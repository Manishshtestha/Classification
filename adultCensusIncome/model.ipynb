{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":498,"sourceType":"datasetVersion","datasetId":225}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:44:58.345591Z","iopub.execute_input":"2025-09-01T08:44:58.345990Z","iopub.status.idle":"2025-09-01T08:44:58.365537Z","shell.execute_reply.started":"2025-09-01T08:44:58.345947Z","shell.execute_reply":"2025-09-01T08:44:58.364515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:44:58.955586Z","iopub.execute_input":"2025-09-01T08:44:58.956040Z","iopub.status.idle":"2025-09-01T08:44:58.961429Z","shell.execute_reply.started":"2025-09-01T08:44:58.956000Z","shell.execute_reply":"2025-09-01T08:44:58.960283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/adult-census-income/adult.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:44:59.559656Z","iopub.execute_input":"2025-09-01T08:44:59.559985Z","iopub.status.idle":"2025-09-01T08:44:59.663173Z","shell.execute_reply.started":"2025-09-01T08:44:59.559959Z","shell.execute_reply":"2025-09-01T08:44:59.662298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:00.124148Z","iopub.execute_input":"2025-09-01T08:45:00.124520Z","iopub.status.idle":"2025-09-01T08:45:00.130961Z","shell.execute_reply.started":"2025-09-01T08:45:00.124490Z","shell.execute_reply":"2025-09-01T08:45:00.130069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:02.000860Z","iopub.execute_input":"2025-09-01T08:45:02.001216Z","iopub.status.idle":"2025-09-01T08:45:02.007581Z","shell.execute_reply.started":"2025-09-01T08:45:02.001189Z","shell.execute_reply":"2025-09-01T08:45:02.006677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:02.392225Z","iopub.execute_input":"2025-09-01T08:45:02.392541Z","iopub.status.idle":"2025-09-01T08:45:02.426287Z","shell.execute_reply.started":"2025-09-01T08:45:02.392518Z","shell.execute_reply":"2025-09-01T08:45:02.425318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.dtypes.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:02.784655Z","iopub.execute_input":"2025-09-01T08:45:02.785647Z","iopub.status.idle":"2025-09-01T08:45:02.795243Z","shell.execute_reply.started":"2025-09-01T08:45:02.785605Z","shell.execute_reply":"2025-09-01T08:45:02.794024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:03.109985Z","iopub.execute_input":"2025-09-01T08:45:03.110382Z","iopub.status.idle":"2025-09-01T08:45:03.142905Z","shell.execute_reply.started":"2025-09-01T08:45:03.110354Z","shell.execute_reply":"2025-09-01T08:45:03.141791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:04.687973Z","iopub.execute_input":"2025-09-01T08:45:04.689221Z","iopub.status.idle":"2025-09-01T08:45:04.717493Z","shell.execute_reply.started":"2025-09-01T08:45:04.689189Z","shell.execute_reply":"2025-09-01T08:45:04.716630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if df.isnull().sum().sum() > 0:\n    import missingno as msno\n    msno.matrix(df)\n    msno.heatmap(df)\nelse:\n    print(\"No missing values — skipping missingno plots.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:04.963716Z","iopub.execute_input":"2025-09-01T08:45:04.964118Z","iopub.status.idle":"2025-09-01T08:45:04.987524Z","shell.execute_reply.started":"2025-09-01T08:45:04.964092Z","shell.execute_reply":"2025-09-01T08:45:04.986325Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df['income'].describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:05.547299Z","iopub.execute_input":"2025-09-01T08:45:05.547681Z","iopub.status.idle":"2025-09-01T08:45:05.560839Z","shell.execute_reply.started":"2025-09-01T08:45:05.547652Z","shell.execute_reply":"2025-09-01T08:45:05.559919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ntarget_counts = df['income'].value_counts()\nplt.Figure(figsize = (10, 6))\nsns.countplot(x=df['income'], data = df)\nplt.title(f\"Class ratio: {target_counts[1]/target_counts[0]:.4f}\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:05.887070Z","iopub.execute_input":"2025-09-01T08:45:05.887400Z","iopub.status.idle":"2025-09-01T08:45:06.064014Z","shell.execute_reply.started":"2025-09-01T08:45:05.887377Z","shell.execute_reply":"2025-09-01T08:45:06.062992Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Numerical Distribution and Outlier Check","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns\n\nfor col in num_cols:\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    fig.suptitle(f\"Distribution Plots for {col} with skewness = {df[col].skew():.2f} \", fontsize=16)\n\n    # Histogram\n    sns.histplot(df[col], bins=30, kde=False, ax=axes[0, 0], color='skyblue')\n    axes[0, 0].set_title('Histogram')\n    axes[0, 0].tick_params(axis='x', rotation=45)\n\n    # KDE plot with fill\n    sns.kdeplot(df[col], fill=True, ax=axes[0, 1], color='violet')\n    axes[0, 1].set_title('KDE Plot (Shaded)')\n    axes[0, 1].tick_params(axis='x', rotation=45)\n\n    # Boxplot\n    sns.boxplot(x=df[col], ax=axes[1, 0], color='skyblue')\n    axes[1, 0].set_title('Boxplot')\n    axes[1, 0].tick_params(axis='x', rotation=45)\n\n    # Violin plot\n    sns.violinplot(x=df[col], ax=axes[1, 1], color='violet')\n    axes[1, 1].set_title('Violin Plot')\n    axes[1, 1].tick_params(axis='x', rotation=45)\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:08.741759Z","iopub.execute_input":"2025-09-01T08:45:08.742491Z","iopub.status.idle":"2025-09-01T08:45:15.590387Z","shell.execute_reply.started":"2025-09-01T08:45:08.742442Z","shell.execute_reply":"2025-09-01T08:45:15.589271Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Categorical Frequency and Relation with Target","metadata":{}},{"cell_type":"code","source":"cat_col = df.select_dtypes(include=['object']).columns\n\nfor col in cat_col:\n    fig, axes = plt.subplots(2, 1, figsize=(20, 10))   \n    fig.suptitle(f\"Categorical Analysis of '{col}'\", fontsize=16)\n\n    sns.countplot(x=col, data=df, ax=axes[0], palette='Set2')\n    axes[0].set_title(f\"{col} Countplot\")\n    axes[0].tick_params(axis='x', rotation=45)  \n\n    sns.countplot(x=col, hue='income', data=df, ax=axes[1])\n    axes[1].set_title(f\"{col} \")\n    axes[1].tick_params(axis='x', rotation=45)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:15.592059Z","iopub.execute_input":"2025-09-01T08:45:15.592444Z","iopub.status.idle":"2025-09-01T08:45:21.050393Z","shell.execute_reply.started":"2025-09-01T08:45:15.592412Z","shell.execute_reply":"2025-09-01T08:45:21.049415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(df['native.country'].value_counts(dropna=False))\nprint(\"\\nNumber of unique values:\", df['native.country'].nunique(dropna=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:21.051398Z","iopub.execute_input":"2025-09-01T08:45:21.051693Z","iopub.status.idle":"2025-09-01T08:45:21.061081Z","shell.execute_reply.started":"2025-09-01T08:45:21.051672Z","shell.execute_reply":"2025-09-01T08:45:21.060016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question_mark_counts = (df == '?').sum()\n\ncols_with_question_marks = question_mark_counts[question_mark_counts > 0]\n\nprint(\"Columns containing '?' and their counts:\")\nprint(cols_with_question_marks)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:21.063172Z","iopub.execute_input":"2025-09-01T08:45:21.063486Z","iopub.status.idle":"2025-09-01T08:45:21.105737Z","shell.execute_reply.started":"2025-09-01T08:45:21.063437Z","shell.execute_reply":"2025-09-01T08:45:21.104806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cols_with_question_mark = ['workclass', 'occupation', 'native.country']\n\nfor col in cols_with_question_mark:\n    df[col] = df[col].replace('?', np.nan)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:21.106603Z","iopub.execute_input":"2025-09-01T08:45:21.106938Z","iopub.status.idle":"2025-09-01T08:45:21.123192Z","shell.execute_reply.started":"2025-09-01T08:45:21.106906Z","shell.execute_reply":"2025-09-01T08:45:21.122189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if df.isnull().sum().sum() > 0:\n    import missingno as msno\n    msno.matrix(df)\n    msno.heatmap(df)\nelse:\n    print(\"No missing values — skipping missingno plots.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:21.124667Z","iopub.execute_input":"2025-09-01T08:45:21.124977Z","iopub.status.idle":"2025-09-01T08:45:22.390041Z","shell.execute_reply.started":"2025-09-01T08:45:21.124943Z","shell.execute_reply":"2025-09-01T08:45:22.389008Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Whenever occupation is missing, workclass is also missing (and vice versa).**","metadata":{}},{"cell_type":"code","source":"df[cols_with_question_mark].isnull().sum()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:22.391188Z","iopub.execute_input":"2025-09-01T08:45:22.391500Z","iopub.status.idle":"2025-09-01T08:45:22.406865Z","shell.execute_reply.started":"2025-09-01T08:45:22.391435Z","shell.execute_reply":"2025-09-01T08:45:22.405964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df['occupation'].isnull() & df['workclass'].notnull()]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:22.408122Z","iopub.execute_input":"2025-09-01T08:45:22.408445Z","iopub.status.idle":"2025-09-01T08:45:22.438073Z","shell.execute_reply.started":"2025-09-01T08:45:22.408414Z","shell.execute_reply":"2025-09-01T08:45:22.437116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Seems Different from my assumption as previous**","metadata":{}},{"cell_type":"code","source":"corr = df[cols_with_question_mark].isnull().astype(int).corr()\nprint(corr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:22.439149Z","iopub.execute_input":"2025-09-01T08:45:22.439903Z","iopub.status.idle":"2025-09-01T08:45:22.457642Z","shell.execute_reply.started":"2025-09-01T08:45:22.439861Z","shell.execute_reply":"2025-09-01T08:45:22.456739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Key Insights:**\n\n**workclass and occupation:**\n\nCorrelation ≈ 0.998 → very strong missingness relationship\n\nThey are almost always missing together\n\nBut not perfectly (as I saw 8 exceptions)\n\n**native.country:**\n\nCorrelation ≈ 0 with the others\n\nMissing independently\n\nShould be treated as separate in imputation strategy","metadata":{}},{"cell_type":"markdown","source":"## Target Mean Plot","metadata":{}},{"cell_type":"code","source":"df['income_binary'] = df['income'].map({'<=50K': 0, '>50K': 1})\n\nfor col in cat_col:\n    target_mean = df.groupby(col)['income_binary'].mean().sort_values(ascending=False)\n\n    plt.Figure(figsize = (14, 8))\n    target_mean.plot(kind='bar', color='skyblue', edgecolor='black')\n    plt.title(f'Mean Income (Probability of >50K) by {col}')\n    plt.ylabel('Proportion of >50K Income')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:22.460179Z","iopub.execute_input":"2025-09-01T08:45:22.460469Z","iopub.status.idle":"2025-09-01T08:45:24.810170Z","shell.execute_reply.started":"2025-09-01T08:45:22.460434Z","shell.execute_reply":"2025-09-01T08:45:24.809140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr = df[num_cols.tolist() + ['income_binary']].corr()['income_binary'].sort_values(ascending=False)\nprint(\"correlation \\n\\n\",corr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:24.811688Z","iopub.execute_input":"2025-09-01T08:45:24.812021Z","iopub.status.idle":"2025-09-01T08:45:24.827707Z","shell.execute_reply.started":"2025-09-01T08:45:24.811992Z","shell.execute_reply":"2025-09-01T08:45:24.826732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Numerical Feature Differences","metadata":{}},{"cell_type":"code","source":"from scipy.stats import ttest_ind\nfor col in num_cols:\n    col0 = df[df['income_binary']== 0 ][col]\n    col1 = df[df['income_binary']== 1 ][col]\n    stat, p = ttest_ind(col0, col1)\n    print(f\"{col} | p-values: {p}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:24.828812Z","iopub.execute_input":"2025-09-01T08:45:24.829188Z","iopub.status.idle":"2025-09-01T08:45:24.888924Z","shell.execute_reply.started":"2025-09-01T08:45:24.829157Z","shell.execute_reply":"2025-09-01T08:45:24.887961Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Variance Inflation Factor","metadata":{}},{"cell_type":"code","source":"# VIF\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n\nX_vif = df[num_cols]\nvif_df = pd.DataFrame({\n    'Feature':X_vif.columns,\n    'VIF': [vif(X_vif.values, i) for i in range(X_vif.shape[1])]    \n})\nprint(vif_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:24.889838Z","iopub.execute_input":"2025-09-01T08:45:24.890085Z","iopub.status.idle":"2025-09-01T08:45:24.937888Z","shell.execute_reply.started":"2025-09-01T08:45:24.890066Z","shell.execute_reply":"2025-09-01T08:45:24.936913Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Education has vif more than 10, so it could be risky and age more than 7**","metadata":{}},{"cell_type":"markdown","source":"## Correlation","metadata":{}},{"cell_type":"code","source":"# Correlation\ncorr = df[num_cols.tolist() + ['income_binary']].corr()\nsns.heatmap(corr, annot = True, cmap = 'coolwarm', fmt = '.2f', linewidth = 0.5)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:24.938812Z","iopub.execute_input":"2025-09-01T08:45:24.939166Z","iopub.status.idle":"2025-09-01T08:45:25.256552Z","shell.execute_reply.started":"2025-09-01T08:45:24.939137Z","shell.execute_reply":"2025-09-01T08:45:25.255528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"demo = df[['education', 'education.num']].drop_duplicates().sort_values('education.num')\nprint(demo)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:25.257698Z","iopub.execute_input":"2025-09-01T08:45:25.258045Z","iopub.status.idle":"2025-09-01T08:45:25.271236Z","shell.execute_reply.started":"2025-09-01T08:45:25.258006Z","shell.execute_reply":"2025-09-01T08:45:25.270253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for tree-based models\ndf1 = df.copy()\ndf1.drop('education.num', axis=1, inplace=True)\n\n# for linear models\ndf2 = df.copy()\ndf2.drop('education', axis=1, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:25.272214Z","iopub.execute_input":"2025-09-01T08:45:25.272512Z","iopub.status.idle":"2025-09-01T08:45:25.302819Z","shell.execute_reply.started":"2025-09-01T08:45:25.272480Z","shell.execute_reply":"2025-09-01T08:45:25.301909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train-Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Tree-based  \nX1 = df1.drop(['income', 'income_binary'], axis=1)\ny1 = df1['income_binary']\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, \nrandom_state=42)\n\nnum_cols1 = X1_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncat_cols1 = X1_train.select_dtypes(include=['object']).columns.tolist()\n\n# Linear model\nX2 = df2.drop(['income', 'income_binary'], axis=1)\ny2 = df2['income_binary']\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, \nrandom_state=42)\n\nnum_cols2 = X2_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncat_cols2 = X2_train.select_dtypes(include=['object']).columns.tolist()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:25.305619Z","iopub.execute_input":"2025-09-01T08:45:25.305909Z","iopub.status.idle":"2025-09-01T08:45:25.351406Z","shell.execute_reply.started":"2025-09-01T08:45:25.305885Z","shell.execute_reply":"2025-09-01T08:45:25.350386Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Outliers detection","metadata":{}},{"cell_type":"code","source":"from scipy.stats import zscore\nimport numpy as np\n\ndef handle_outliers(X_train, num_cols, dataset_name=\"\"):\n    print(f\"--- Outlier handling for {dataset_name} ---\\n\")\n    \n    for col in num_cols:\n        if col != 'income_binary':\n            # Handle skewed features with IQR\n            if abs(X_train[col].skew()) > 1:\n                Q1, Q3 = X_train[col].quantile([0.25, 0.75])\n                IQR = Q3 - Q1\n                lower, upper = Q1 - 3*IQR, Q3 + 3*IQR\n                outliers_b = X_train[(X_train[col] < lower) | (X_train[col] > upper)]\n                X_train[col] = X_train[col].where((X_train[col] >= lower) & (X_train[col] <= upper))\n                outliers_a = X_train[(X_train[col] < lower) | (X_train[col] > upper)]\n            else:\n                # Use z-score for less skewed data\n                z_scores = zscore(X_train[col].dropna())\n                outliers_b = X_train.loc[np.abs(z_scores) > 3]\n                X_train[col] = X_train[col].where(np.abs(z_scores) <= 3)\n                outliers_a = X_train.loc[np.abs(z_scores) > 3]\n\n            print(f\"{col}: {len(outliers_b)} outliers before\")\n            print(f\"{col}: {len(outliers_a)} outliers after\\n\")\n\n# for  tree-based training data\nhandle_outliers(X1_train, num_cols1, dataset_name=\"Tree-Based (X1_train)\")\n\n# for  linear-model training data\nhandle_outliers(X2_train, num_cols2, dataset_name=\"Linear Model (X2_train)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:26.291583Z","iopub.execute_input":"2025-09-01T08:45:26.291919Z","iopub.status.idle":"2025-09-01T08:45:26.351055Z","shell.execute_reply.started":"2025-09-01T08:45:26.291884Z","shell.execute_reply":"2025-09-01T08:45:26.349998Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Pipeline","metadata":{}},{"cell_type":"markdown","source":"## Linear Model Pipeline","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\n# For df2 (linear)\nnum_pipe_linear = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncat_pipe_linear = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor_linear = ColumnTransformer([\n    ('num', num_pipe_linear, num_cols2),\n    ('cat', cat_pipe_linear, cat_cols2)\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:32.088488Z","iopub.execute_input":"2025-09-01T08:45:32.089261Z","iopub.status.idle":"2025-09-01T08:45:32.095323Z","shell.execute_reply.started":"2025-09-01T08:45:32.089228Z","shell.execute_reply":"2025-09-01T08:45:32.094276Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tree-Based Model Pipeline","metadata":{}},{"cell_type":"code","source":"# For df1 (tree-based)\nnum_pipe_tree = Pipeline([\n    ('imputer', SimpleImputer(strategy='median'))\n])\n\ncat_pipe_tree = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n])\n\npreprocessor_tree = ColumnTransformer([\n    ('num', num_pipe_tree, num_cols1),\n    ('cat', cat_pipe_tree, cat_cols1)\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:33.129053Z","iopub.execute_input":"2025-09-01T08:45:33.129420Z","iopub.status.idle":"2025-09-01T08:45:33.135414Z","shell.execute_reply.started":"2025-09-01T08:45:33.129394Z","shell.execute_reply":"2025-09-01T08:45:33.134489Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Fitting","metadata":{}},{"cell_type":"code","source":"import os\nimport joblib\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nmodels = {\n    'RandomForest': RandomForestClassifier(random_state=42),\n    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n    'KNN': KNeighborsClassifier(),\n    'DecisionTree': DecisionTreeClassifier(random_state=42),\n    'NaiveBayes': GaussianNB(),\n    'LinearSVC': LinearSVC(random_state=42, max_iter=10000),\n    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n    'LightGBM': LGBMClassifier(random_state=42),\n    'CatBoost': CatBoostClassifier(verbose=0, random_state=42)\n}\n\nsave_dir = \"/kaggle/working/base_models\"\nos.makedirs(save_dir, exist_ok=True)\n\n# Train and save each model\nfor name, model in models.items():\n    try:\n        print(f\"Training {name}...\")\n\n        # Determine pipeline type\n        if name in ['LogisticRegression', 'LinearSVC', 'KNN', 'NaiveBayes']:\n            X_train, y_train = X2_train, y2_train\n            preprocessor = preprocessor_linear\n        else:\n            X_train, y_train = X1_train, y1_train\n            preprocessor = preprocessor_tree\n\n        # Build pipeline\n        pipeline = Pipeline([\n            ('preprocessor', preprocessor),\n            ('classifier', model)\n        ])\n\n        pipeline.fit(X_train, y_train)\n        model_path = os.path.join(save_dir, f\"{name}.joblib\")\n        joblib.dump(pipeline, model_path)\n\n        print(f\"{name} saved successfully to {model_path}\\n\")\n\n    except Exception as e:\n        print(f\" Failed to train {name}: {e}\\n\")\n\n# Save test sets once\njoblib.dump(X1_test, \"X1_test.joblib\")\njoblib.dump(y1_test, \"y1_test.joblib\")\njoblib.dump(X2_test, \"X2_test.joblib\")\njoblib.dump(y2_test, \"y2_test.joblib\")\n\nprint(f\"Models saved to: {save_dir}\")\nprint(\"Files in save_dir:\", os.listdir(save_dir))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:39.202335Z","iopub.execute_input":"2025-09-01T08:45:39.202690Z","iopub.status.idle":"2025-09-01T08:45:51.261919Z","shell.execute_reply.started":"2025-09-01T08:45:39.202664Z","shell.execute_reply":"2025-09-01T08:45:51.260812Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evalutation","metadata":{}},{"cell_type":"code","source":"import os\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n\nsave_dir = \"/kaggle/working/base_models\"\n\n# Load test sets\nX1_test = joblib.load(\"X1_test.joblib\")\ny1_test = joblib.load(\"y1_test.joblib\")\n\nX2_test = joblib.load(\"X2_test.joblib\")\ny2_test = joblib.load(\"y2_test.joblib\")\n\nmodel_files = [f for f in os.listdir(save_dir) if f.endswith(\".joblib\")]\n\n\ntree_based_models = ['RandomForest.joblib', 'DecisionTree.joblib', 'LightGBM.joblib', 'CatBoost.joblib']\nlinear_models = ['LogisticRegression.joblib', 'LinearSVC.joblib', 'KNN.joblib']\n\nfor model_file in model_files:\n    print(f\"Evaluating {model_file} ...\")\n    pipeline = joblib.load(os.path.join(save_dir, model_file))\n\n    # Choose the correct test data\n    if model_file in tree_based_models:\n        X_test, y_test = X1_test, y1_test\n    elif model_file in linear_models:\n        X_test, y_test = X2_test, y2_test\n    else:\n        print(f\"Model {model_file} not assigned to a test set, skipping...\\n\")\n        continue\n\n\n     # Check if loaded object is Pipeline and has 'classifier' step\n    if hasattr(pipeline, 'named_steps') and 'classifier' in pipeline.named_steps:\n        classifier = pipeline.named_steps['classifier']\n        y_pred = pipeline.predict(X_test)\n        # Get probabilities or decision function if available\n        if hasattr(classifier, \"predict_proba\"):\n            y_proba = pipeline.predict_proba(X_test)[:, 1]\n        elif hasattr(classifier, \"decision_function\"):\n            y_proba = pipeline.decision_function(X_test)\n            from sklearn.preprocessing import MinMaxScaler\n            scaler = MinMaxScaler()\n            y_proba = scaler.fit_transform(y_proba.reshape(-1,1)).ravel()\n        else:\n            y_proba = None\n    else:\n        # Fallback: treat pipeline as model itself\n        y_pred = pipeline.predict(X_test)\n        if hasattr(pipeline, \"predict_proba\"):\n            y_proba = pipeline.predict_proba(X_test)[:, 1]\n        elif hasattr(pipeline, \"decision_function\"):\n            y_proba = pipeline.decision_function(X_test)\n            from sklearn.preprocessing import MinMaxScaler\n            scaler = MinMaxScaler()\n            y_proba = scaler.fit_transform(y_proba.reshape(-1,1)).ravel()\n        else:\n            y_proba = None\n            \n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred)\n    rec = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n\n    print(f\"Accuracy:  {acc:.4f}\")\n    print(f\"Precision: {prec:.4f}\")\n    print(f\"Recall:    {rec:.4f}\")\n    print(f\"F1 Score:  {f1:.4f}\")\n\n    # Confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(5,4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title(f\"Confusion Matrix: {model_file}\")\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n\n    # ROC AUC curve if possible\n    if y_proba is not None:\n        auc = roc_auc_score(y_test, y_proba)\n        fpr, tpr, _ = roc_curve(y_test, y_proba)\n        plt.figure(figsize=(6,5))\n        plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n        plt.plot([0,1], [0,1], 'k--')\n        plt.title(f'ROC Curve: {model_file}')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.legend()\n        plt.show()\n\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:45:57.077859Z","iopub.execute_input":"2025-09-01T08:45:57.078220Z","iopub.status.idle":"2025-09-01T08:46:21.888252Z","shell.execute_reply.started":"2025-09-01T08:45:57.078196Z","shell.execute_reply":"2025-09-01T08:46:21.886710Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Learning Curve","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import learning_curve\nfrom sklearn.metrics import make_scorer, f1_score\n\n\nmodel_files = [f for f in os.listdir(save_dir) if f.endswith(\".joblib\")]\n\ntree_based_models = ['RandomForest.joblib', 'DecisionTree.joblib']\nlinear_models = ['LogisticRegression.joblib', 'LinearSVC.joblib', 'KNN.joblib']\n\ndef plot_learning_curve(estimator, X, y, title):\n    train_sizes, train_scores, val_scores = learning_curve(\n        estimator, X, y,\n        cv=5,\n        scoring=make_scorer(f1_score),\n        n_jobs=-1,\n        train_sizes=np.linspace(0.1, 1.0, 10),\n        verbose=0\n    )\n\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    val_mean = np.mean(val_scores, axis=1)\n    val_std = np.std(val_scores, axis=1)\n\n    plt.figure(figsize=(8,6))\n    plt.plot(train_sizes, train_mean, 'o-', color='blue', label=\"Training F1 score\")\n    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n\n    plt.plot(train_sizes, val_mean, 'o-', color='green', label=\"Cross-validation F1 score\")\n    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='green')\n\n    plt.title(title)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"F1 Score\")\n    plt.legend(loc=\"best\")\n    plt.grid(True)\n    plt.show()\n\nfor model_file in model_files:\n    print(f\"Processing learning curve for {model_file} ...\")\n    pipeline = joblib.load(os.path.join(save_dir, model_file))\n\n    # Select train data corresponding to model type\n    if model_file in tree_based_models:\n        X_train, y_train = X1_test, y1_test   \n    elif model_file in linear_models:\n        X_train, y_train = X2_test, y2_test\n    else:\n        print(f\"Skipping {model_file} as it doesn't match model groups.\")\n        continue\n\n    try:\n        plot_learning_curve(pipeline, X_train, y_train, title=f\"Learning Curve for {model_file}\")\n    except Exception as e:\n        print(f\"Failed to plot learning curve for {model_file}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:46:30.290425Z","iopub.execute_input":"2025-09-01T08:46:30.290779Z","iopub.status.idle":"2025-09-01T08:47:10.300696Z","shell.execute_reply.started":"2025-09-01T08:46:30.290755Z","shell.execute_reply":"2025-09-01T08:47:10.298818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}